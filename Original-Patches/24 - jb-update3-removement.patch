diff --git a/drivers/net/pppopns.c b/drivers/net/pppopns.c
index fb81984..93de2d2 100644
--- a/drivers/net/pppopns.c
+++ b/drivers/net/pppopns.c
@@ -119,7 +119,8 @@ static int pppopns_recv_core(struct sock *sk_raw, struct sk_buff *skb)
 		goto drop;
 
 	/* Perform reordering if sequencing is enabled. */
-	if (hdr->bits & PPTP_GRE_SEQ_BIT) {
+	if ((opt->ppp_flags & SC_GRE_SEQ_CHK) &&
+	    (hdr->bits & PPTP_GRE_SEQ_BIT)) {
 		struct sk_buff *skb1;
 
 		/* Insert the packet into receive queue in order. */
@@ -378,6 +379,8 @@ static struct proto_ops pppopns_proto_ops = {
 static int pppopns_create(struct net *net, struct socket *sock)
 {
 	struct sock *sk;
+	struct pppox_sock *po;
+	struct pppopns_opt *opt;
 
 	sk = sk_alloc(net, PF_PPPOX, GFP_KERNEL, &pppopns_proto);
 	if (!sk)
@@ -388,13 +391,49 @@ static int pppopns_create(struct net *net, struct socket *sock)
 	sock->ops = &pppopns_proto_ops;
 	sk->sk_protocol = PX_PROTO_OPNS;
 	sk->sk_state = PPPOX_NONE;
+
+	po = pppox_sk(sk);
+	opt = &po->proto.pns;
+	opt->ppp_flags = SC_GRE_SEQ_CHK;
+
 	return 0;
 }
 
+static int pppopns_ioctl(struct socket *sock, unsigned int cmd, unsigned long arg)
+{
+
+        struct sock *sk = sock->sk;
+	struct pppox_sock *po = pppox_sk(sk);
+	struct pppopns_opt *opt = &po->proto.pns;
+	void __user *argp = (void __user *)arg;
+	int __user *p = argp;
+	int err = -ENOTTY, val;
+
+	switch (cmd) {
+	case PPPIOCGFLAGS:
+		printk("Getting pppopns socket flags.\n");
+		val = opt->ppp_flags;
+		if (put_user(val, p))
+			break;
+		err = 0;
+		break;
+	case PPPIOCSFLAGS:
+		printk("Setting pppopns socket flags.\n");
+		if (get_user(val, p))
+			break;
+		opt->ppp_flags = val;
+		err = 0;
+		break;
+	}
+
+	return err;
+}
+
 /******************************************************************************/
 
 static struct pppox_proto pppopns_pppox_proto = {
 	.create = pppopns_create,
+    .ioctl = pppopns_ioctl,
 	.owner = THIS_MODULE,
 };
 
diff --git a/drivers/net/wireless/bcmdhd/wl_cfgp2p.c b/drivers/net/wireless/bcmdhd/wl_cfgp2p.c
index ac41a97..220b2a2 100644
--- a/drivers/net/wireless/bcmdhd/wl_cfgp2p.c
+++ b/drivers/net/wireless/bcmdhd/wl_cfgp2p.c
@@ -1164,8 +1164,8 @@ wl_cfgp2p_set_management_ie(struct wl_priv *wl, struct net_device *ndev, s32 bss
 				parsed_ie_buf_len += vndrie_info->ie_len;
 			}
 		}
-
-		if (mgmt_ie_buf != NULL) {
+		CFGP2P_ERR(("### CSP 675188 debug_log %s(%d)\n", __func__, __LINE__));
+		if (mgmt_ie_buf != NULL || mgmt_ie_len != NULL) {
 			if (parsed_ie_buf_len && (parsed_ie_buf_len == *mgmt_ie_len) &&
 			     (memcmp(mgmt_ie_buf, curr_ie_buf, parsed_ie_buf_len) == 0)) {
 				CFGP2P_INFO(("Previous mgmt IE is equals to current IE"));
diff --git a/drivers/power/bat_manager.c b/drivers/power/bat_manager.c
index 6c56c01..1f343bc 100644
--- a/drivers/power/bat_manager.c
+++ b/drivers/power/bat_manager.c
@@ -34,18 +34,16 @@
 #include <linux/bat_manager.h>
 #include <linux/gpio.h>
 
-#define DEBUG_PRINT 0
-
 #define FAST_POLL	(1 * 40)
 #define SLOW_POLL	(30 * 60)
 
-#define STATUS_BATT_CHARGABLE           0x0
+#define STATUS_BATT_CHARGABLE        	0x0
 #define STATUS_BATT_FULL		0x1
 #define STATUS_BATT_CHARGE_TIME_OUT	0x2
 #define STATUS_BATT_ABNOMAL_TEMP	0x4
 
-#define STABLE_LOW_BATTERY_DIFF		3
-#define STABLE_LOW_BATTERY_DIFF_LOWBATT	1
+#define STABLE_LOW_BATTERY_DIFF		 3
+#define STABLE_LOW_BATTERY_DIFF_LOWBATT	 1
 
 struct charger_device_info {
 	struct device		*dev;
@@ -119,19 +117,16 @@ static void set_full_charge(struct battery_manager_callbacks *ptr)
 
 static int fuelgauge_recovery(struct charger_device_info *di)
 {
-	int current_soc;
+	int current_soc = 0;
 
 	if (di->bat_info.soc > 0) {
-		pr_err("%s: Reduce the Reported SOC by 1 unit, wait for 30s\n",
-				__func__);
+		pr_err("%s: Reduce the Reported SOC by 1 unit, wait for 30s\n", __func__);
 		if (di->pdata->get_fuel_value)
 			current_soc = di->pdata->get_fuel_value(READ_FG_SOC);
 
 		if (current_soc) {
-			pr_info("%s: Returning to Normal discharge path.\n",
-					__func__);
-			pr_info(" Actual SOC(%d) non-zero.\n",
-					current_soc);
+			pr_info("%s: Returning to Normal discharge path.\n", __func__);
+			pr_info(" Actual SOC(%d) non-zero.\n", current_soc);
 			di->is_low_batt_alarm = false;
 			goto return_soc;
 		} else {
@@ -167,8 +162,7 @@ static void batman_low_battery_alarm(struct charger_device_info *di)
 	if ((di->bat_info.soc - current_soc) > overcurrent_limit_in_soc) {
 		pr_info("%s: Abnormal Current Consumption jump by %d units.\n",
 			__func__, ((di->bat_info.soc - current_soc)));
-		pr_info("Last Reported SOC (%d).\n",
-				di->bat_info.soc);
+		pr_info("Last Reported SOC (%d).\n", di->bat_info.soc);
 
 		di->is_low_batt_alarm = true;
 		wake_lock(&di->work_wake_lock);
@@ -422,7 +416,7 @@ static void charger_detect_work(struct work_struct *work)
 static int batman_get_bat_level(struct charger_device_info *di)
 {
 	int soc = 0;
-
+	int prev_soc = di->bat_info.soc;
 	/* check VFcapacity every five minutes */
 	if (!(di->fg_chk_cnt++ % 10)) {
 		di->pdata->check_vf_fullcap_range();
@@ -434,6 +428,12 @@ static int batman_get_bat_level(struct charger_device_info *di)
 			di->pdata->get_fuel_value(READ_FG_SOC);
 		if (!di->is_full_charged && soc > 99 && di->is_cable_attached)
 			soc = 99;
+
+		/* The following condition is a handling in software for current soc
+		   to not be less than prev_soc when charging source is detached*/
+		if(prev_soc < soc && prev_soc > 98 && !di->is_cable_attached)
+			soc=prev_soc;
+
 		di->bat_info.soc = soc;
 	}
 
diff --git a/drivers/staging/android/logger.c b/drivers/staging/android/logger.c
index 53d0190..3963187 100644
--- a/drivers/staging/android/logger.c
+++ b/drivers/staging/android/logger.c
@@ -540,12 +540,7 @@ static int logger_release(struct inode *ignored, struct file *file)
 {
 	if (file->f_mode & FMODE_READ) {
 		struct logger_reader *reader = file->private_data;
-		struct logger_log *log = reader->log;
-		
-		mutex_lock(&log->mutex);
 		list_del(&reader->list);
-		mutex_unlock(&log->mutex);
-		
 		kfree(reader);
 	}
 
diff --git a/drivers/staging/android/lowmemorykiller.c b/drivers/staging/android/lowmemorykiller.c
index 74957fc..d0666a1 100644
--- a/drivers/staging/android/lowmemorykiller.c
+++ b/drivers/staging/android/lowmemorykiller.c
@@ -38,44 +38,7 @@
 #include <linux/oom.h>
 #include <linux/sched.h>
 #include <linux/notifier.h>
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-#include <linux/fs.h>
-#include <linux/swap.h>
-#include <linux/device.h>
-#include <linux/err.h>
-#include <linux/mm_inline.h>
-#include <linux/kthread.h>
-#include <linux/freezer.h>
-#include <linux/cpu.h>
-#include <asm/atomic.h>
 
-#if defined(CONFIG_SMP)
-#define NR_TO_RECLAIM_PAGES 		(1024 * NR_CPUS)/* 4MB*cpu_core, include file pages */
-#define MIN_FREESWAP_PAGES 		(NR_TO_RECLAIM_PAGES*2) /* 4MB*cpu_core*2 */
-#define MIN_RECLAIM_PAGES 		(NR_TO_RECLAIM_PAGES/8)
-#define MIN_CSWAP_INTERVAL 		(10*HZ) /* 10 senconds */
-#else /* CONFIG_SMP */
-#define NR_TO_RECLAIM_PAGES 		1024 /* 4MB, include file pages */
-#define MIN_FREESWAP_PAGES 		(NR_TO_RECLAIM_PAGES*2)  /* 4MB*2 */
-#define MIN_RECLAIM_PAGES 		(NR_TO_RECLAIM_PAGES/8)
-#define MIN_CSWAP_INTERVAL 		(10*HZ) /* 10 senconds */
-#endif
-
-struct soft_reclaim {
-	atomic_t kcompcached_running;
-	atomic_t need_to_reclaim;
-	atomic_t lmk_running;
-	struct task_struct *kcompcached;
-};
-
-static struct soft_reclaim s_reclaim;
-extern atomic_t kswapd_thread_on;
-static unsigned long prev_jiffy;
-static uint32_t number_of_reclaim_pages = NR_TO_RECLAIM_PAGES;
-static uint32_t minimum_freeswap_pages = MIN_FREESWAP_PAGES;
-static uint32_t minimum_reclaim_pages = MIN_RECLAIM_PAGES;
-static uint32_t minimum_interval_time = MIN_CSWAP_INTERVAL;
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 #define LMK_COUNT_READ
 
 #ifdef CONFIG_ENHANCED_LMK_ROUTINE

@@ -174,10 +137,6 @@ static int lowmem_shrink(struct shrinker *s, struct shrink_control *sc)
 	int other_file = global_page_state(NR_FILE_PAGES) -
 						global_page_state(NR_SHMEM);
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	other_file -= total_swapcache_pages;
-#endif
-
 	/*
 	 * If we already have a death outstanding, then
 	 * bail out right away; indicating to vmscan
@@ -229,9 +182,6 @@ static int lowmem_shrink(struct shrinker *s, struct shrink_control *sc)
 	selected_oom_adj = min_adj;
 #endif
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	atomic_set(&s_reclaim.lmk_running, 1);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	read_lock(&tasklist_lock);
 	for_each_process(p) {
 		struct mm_struct *mm;
@@ -324,109 +262,10 @@ static int lowmem_shrink(struct shrinker *s, struct shrink_control *sc)
 #endif
 	}
 #endif
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	atomic_set(&s_reclaim.lmk_running, 0);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	return rem;
 }
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-void could_cswap(void)
-{
-	if (atomic_read(&s_reclaim.need_to_reclaim) == 0)
-		return;
-
-	if (time_before(jiffies, prev_jiffy + minimum_interval_time))
-		return;
-
-	if (atomic_read(&s_reclaim.lmk_running) == 1 || atomic_read(&kswapd_thread_on) == 1) 
-		return;
-
-	if (nr_swap_pages < minimum_freeswap_pages)
-		return;
-
-	if (idle_cpu(task_cpu(s_reclaim.kcompcached)) && this_cpu_loadx(4) == 0) {
-		if (atomic_read(&s_reclaim.kcompcached_running) == 0) {
-			wake_up_process(s_reclaim.kcompcached);
-			atomic_set(&s_reclaim.kcompcached_running, 1);
-			prev_jiffy = jiffies;
-		}
-	}
-}
-
-inline void need_soft_reclaim(void)
-{
-	atomic_set(&s_reclaim.need_to_reclaim, 1);
-}
-
-inline void cancel_soft_reclaim(void)
-{
-	atomic_set(&s_reclaim.need_to_reclaim, 0);
-}
-
-int get_soft_reclaim_status(void)
-{
-	int kcompcache_running = atomic_read(&s_reclaim.kcompcached_running);
-	return kcompcache_running;
-}
-
-extern long rtcc_reclaim_pages(long nr_to_reclaim);
-static int do_compcache(void * nothing)
-{
-	int ret;
-	set_freezable();
-
-	for ( ; ; ) {
-		ret = try_to_freeze();
-		if (kthread_should_stop())
-			break;
-
-		if (rtcc_reclaim_pages(number_of_reclaim_pages) < minimum_reclaim_pages)
-			cancel_soft_reclaim();
-
-		atomic_set(&s_reclaim.kcompcached_running, 0);
-		set_current_state(TASK_INTERRUPTIBLE);
-		schedule();
-	}
-
-	return 0;
-}
-
-static ssize_t rtcc_trigger_store(struct class *class, struct class_attribute *attr,
-			const char *buf, size_t count)
-{
-	long val, magic_sign;
-
-	sscanf(buf, "%ld,%ld", &val, &magic_sign);
-
-	if (val < 0 || ((val * val - 1) != magic_sign)) {
-		pr_warning("Invalid command.\n");
-		goto out;
-	}
-
-	need_soft_reclaim();
-
-out:
-	return count;
-}
-static CLASS_ATTR(rtcc_trigger, 0200, NULL, rtcc_trigger_store);
-static struct class *kcompcache_class;
-
-static int kcompcache_idle_notifier(struct notifier_block *nb, unsigned long val, void *data)
-{
-	could_cswap();
-	return 0;
-}
-
-static struct notifier_block kcompcache_idle_nb = {
-	.notifier_call = kcompcache_idle_notifier,
-};
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
-
 static struct shrinker lowmem_shrinker = {
 	.shrink = lowmem_shrink,
 	.seeks = DEFAULT_SEEKS * 16
@@ -436,29 +275,6 @@ static int __init lowmem_init(void)
 {
 	task_free_register(&task_nb);
 	register_shrinker(&lowmem_shrinker);
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	s_reclaim.kcompcached = kthread_run(do_compcache, NULL, "kcompcached");
-	if (IS_ERR(s_reclaim.kcompcached)) {
-		/* failure at boot is fatal */
-		BUG_ON(system_state == SYSTEM_BOOTING); 
-	}
-	set_user_nice(s_reclaim.kcompcached, 0);
-	atomic_set(&s_reclaim.need_to_reclaim, 0);
-	atomic_set(&s_reclaim.kcompcached_running, 0);
-	prev_jiffy = jiffies;
-
-	idle_notifier_register(&kcompcache_idle_nb);
-
-	kcompcache_class = class_create(THIS_MODULE, "kcompcache");
-	if (IS_ERR(kcompcache_class)) {
-		pr_err("%s: couldn't create kcompcache class.\n", __func__);
-		return 0;
-	}
-	if (class_create_file(kcompcache_class, &class_attr_rtcc_trigger) < 0) {
-		pr_err("%s: couldn't create rtcc trigger sysfs file.\n", __func__);
-		class_destroy(kcompcache_class);
-	}
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	return 0;
 }
 
@@ -466,20 +282,6 @@ static void __exit lowmem_exit(void)
 {
 	unregister_shrinker(&lowmem_shrinker);
 	task_free_unregister(&task_nb);
-
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	idle_notifier_unregister(&kcompcache_idle_nb);
-	if (s_reclaim.kcompcached) {
-		cancel_soft_reclaim();
-		kthread_stop(s_reclaim.kcompcached);
-		s_reclaim.kcompcached = NULL;
-	}
-
-	if (kcompcache_class) {
-		class_remove_file(kcompcache_class, &class_attr_rtcc_trigger);
-		class_destroy(kcompcache_class);
-	}
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 }
 
 module_param_named(cost, lowmem_shrinker.seeks, int, S_IRUGO | S_IWUSR);
@@ -493,14 +295,7 @@ module_param_named(debug_level, lowmem_debug_level, uint, S_IRUGO | S_IWUSR);
 module_param_named(lmkcount, lmk_count, uint, S_IRUGO);
 #endif
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-module_param_named(nr_reclaim, number_of_reclaim_pages, uint, S_IRUSR | S_IWUSR);
-module_param_named(min_freeswap, minimum_freeswap_pages, uint, S_IRUSR | S_IWUSR);
-module_param_named(min_reclaim, minimum_reclaim_pages, uint, S_IRUSR | S_IWUSR);
-module_param_named(min_interval, minimum_interval_time, uint, S_IRUSR | S_IWUSR);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 module_init(lowmem_init);
 module_exit(lowmem_exit);
 
 MODULE_LICENSE("GPL");
-
diff --git a/drivers/staging/ath6kl/Makefile b/drivers/staging/ath6kl/Makefile
index 1d3f239..a2110f1 100644
--- a/drivers/staging/ath6kl/Makefile
+++ b/drivers/staging/ath6kl/Makefile
@@ -2,7 +2,7 @@
 # Copyright (c) 2004-2010 Atheros Communications Inc.
 # All rights reserved.
 #
-# 
+#
 #
 # Permission to use, copy, modify, and/or distribute this software for any
 # purpose with or without fee is hereby granted, provided that the above
@@ -57,8 +57,6 @@ ccflags-y += -DSETUPBTDEV_ENABLED
 ath6kl-y += htc2/AR6000/ar6k_gmbox.o
 ath6kl-y += htc2/AR6000/ar6k_gmbox_hciuart.o
 ath6kl-y += miscdrv/ar3kconfig.o
-ath6kl-y += miscdrv/ar3kps/ar3kpsconfig.o
-ath6kl-y += miscdrv/ar3kps/ar3kpsparser.o
 endif
 
 ifeq ($(CONFIG_ATH6KL_CONFIG_GPIO_BT_RESET),y)
diff --git a/drivers/staging/zram/Kconfig b/drivers/staging/zram/Kconfig
index 06f741a..3bec4db 100644
--- a/drivers/staging/zram/Kconfig
+++ b/drivers/staging/zram/Kconfig
@@ -28,10 +28,3 @@ config ZRAM_DEBUG
 	help
 	  This option adds additional debugging code to the compressed
 	  RAM block device driver.
-
-config ZRAM_FOR_ANDROID
-	bool "Optimize zram behavior for android"
-	depends on ZRAM && ANDROID
-	default n
-	help
-	  This option enables modified zram behavior optimized for android
diff --git a/drivers/staging/zram/zram_drv.c b/drivers/staging/zram/zram_drv.c
index c843d93..aab4ec4 100644
--- a/drivers/staging/zram/zram_drv.c
+++ b/drivers/staging/zram/zram_drv.c
@@ -37,7 +37,7 @@
 
 /* Globals */
 static int zram_major;
-struct zram *zram_devices;
+struct zram *devices;
 
 /* Module params (documentation at end) */
 unsigned int num_devices;
@@ -678,14 +678,14 @@ static int __init zram_init(void)
 
 	/* Allocate the device array and initialize each one */
 	pr_info("Creating %u devices ...\n", num_devices);
-	zram_devices = kzalloc(num_devices * sizeof(struct zram), GFP_KERNEL);
-	if (!zram_devices) {
+	devices = kzalloc(num_devices * sizeof(struct zram), GFP_KERNEL);
+	if (!devices) {
 		ret = -ENOMEM;
 		goto unregister;
 	}
 
 	for (dev_id = 0; dev_id < num_devices; dev_id++) {
-		ret = create_device(&zram_devices[dev_id], dev_id);
+		ret = create_device(&devices[dev_id], dev_id);
 		if (ret)
 			goto free_devices;
 	}
@@ -694,8 +694,8 @@ static int __init zram_init(void)
 
 free_devices:
 	while (dev_id)
-		destroy_device(&zram_devices[--dev_id]);
-	kfree(zram_devices);
+		destroy_device(&devices[--dev_id]);
+	kfree(devices);
 unregister:
 	unregister_blkdev(zram_major, "zram");
 out:
@@ -708,7 +708,7 @@ static void __exit zram_exit(void)
 	struct zram *zram;
 
 	for (i = 0; i < num_devices; i++) {
-		zram = &zram_devices[i];
+		zram = &devices[i];
 
 		destroy_device(zram);
 		if (zram->init_done)
@@ -717,7 +717,7 @@ static void __exit zram_exit(void)
 
 	unregister_blkdev(zram_major, "zram");
 
-	kfree(zram_devices);
+	kfree(devices);
 	pr_debug("Cleanup done!\n");
 }
 
diff --git a/drivers/staging/zram/zram_drv.h b/drivers/staging/zram/zram_drv.h
index 3ad9486..408b2c0 100644
--- a/drivers/staging/zram/zram_drv.h
+++ b/drivers/staging/zram/zram_drv.h
@@ -120,7 +120,7 @@ struct zram {
 	struct zram_stats stats;
 };
 
-extern struct zram *zram_devices;
+extern struct zram *devices;
 extern unsigned int num_devices;
 #ifdef CONFIG_SYSFS
 extern struct attribute_group zram_disk_attr_group;
diff --git a/drivers/staging/zram/zram_sysfs.c b/drivers/staging/zram/zram_sysfs.c
index 753f13c..a70cc01 100644
--- a/drivers/staging/zram/zram_sysfs.c
+++ b/drivers/staging/zram/zram_sysfs.c
@@ -35,7 +35,7 @@ static struct zram *dev_to_zram(struct device *dev)
 	struct zram *zram = NULL;
 
 	for (i = 0; i < num_devices; i++) {
-		zram = &zram_devices[i];
+		zram = &devices[i];
 		if (disk_to_dev(zram->disk) == dev)
 			break;
 	}
@@ -218,4 +218,4 @@ static struct attribute *zram_disk_attrs[] = {
 
 struct attribute_group zram_disk_attr_group = {
 	.attrs = zram_disk_attrs,
-};
\ No newline at end of file
+};
diff --git a/include/linux/if_ppp.h b/include/linux/if_ppp.h
index c9ad383..c2bea12 100644
--- a/include/linux/if_ppp.h
+++ b/include/linux/if_ppp.h
@@ -45,7 +45,6 @@
 #define PROTO_IPX	0x002b	/* protocol numbers */
 #define PROTO_DNA_RT    0x0027  /* DNA Routing */
 
-
 /*
  * Bit definitions for flags.
  */
@@ -65,6 +64,7 @@
 #define SC_COMP_RUN	0x00001000	/* compressor has been inited */
 #define SC_DECOMP_RUN	0x00002000	/* decompressor has been inited */
 #define SC_MP_XSHORTSEQ	0x00004000	/* transmit short MP seq numbers */
+#define SC_GRE_SEQ_CHK  0x00008000      /* enable GRE sequencing in pppopns */
 #define SC_DEBUG	0x00010000	/* enable debug messages */
 #define SC_LOG_INPKT	0x00020000	/* log contents of good pkts recvd */
 #define SC_LOG_OUTPKT	0x00040000	/* log contents of pkts sent */
diff --git a/include/linux/if_pppox.h b/include/linux/if_pppox.h
index cffe2b2..1ee1fc6 100644
--- a/include/linux/if_pppox.h
+++ b/include/linux/if_pppox.h
@@ -1,6 +1,6 @@
 /***************************************************************************
  * Linux PPP over X - Generic PPP transport layer sockets
- * Linux PPP over Ethernet (PPPoE) Socket Implementation (RFC 2516) 
+ * Linux PPP over Ethernet (PPPoE) Socket Implementation (RFC 2516)
  *
  * This file supplies definitions required by the PPP over Ethernet driver
  * (pppox.c).  All version information wrt this file is located in pppox.c
@@ -16,7 +16,6 @@
 #ifndef __LINUX_IF_PPPOX_H
 #define __LINUX_IF_PPPOX_H
 
-
 #include <linux/types.h>
 #include <asm/byteorder.h>
 
@@ -38,17 +37,17 @@
 #define PF_PPPOX	AF_PPPOX
 #endif /* !(AF_PPPOX) */
 
-/************************************************************************ 
- * PPPoE addressing definition 
- */ 
+/************************************************************************
+ * PPPoE addressing definition
+ */
 typedef __be16 sid_t;
 struct pppoe_addr {
 	sid_t         sid;                    /* Session identifier */
 	unsigned char remote[ETH_ALEN];       /* Remote address */
 	char          dev[IFNAMSIZ];          /* Local device to use */
-}; 
- 
-/************************************************************************ 
+};
+
+/************************************************************************
  * PPTP addressing definition
  */
 struct pptp_addr {
@@ -188,6 +187,7 @@ struct pppopns_opt {
 	__u32		xmit_sequence;
 	void		(*data_ready)(struct sock *sk_raw, int length);
 	int		(*backlog_rcv)(struct sock *sk_raw, struct sk_buff *skb);
+	int 		ppp_flags;
 };
 
 #include <net/sock.h>
diff --git a/include/linux/sched.h b/include/linux/sched.h
index c64c997..ec422c1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -33,7 +33,7 @@
 /*
  * Scheduling policies
  */
-#define SCHED_NORMAL		0
+#define SCHED_NORMAL	0
 #define SCHED_FIFO		1
 #define SCHED_RR		2
 #define SCHED_BATCH		3
@@ -141,9 +141,7 @@ extern unsigned long nr_uninterruptible(void);
 extern unsigned long nr_iowait(void);
 extern unsigned long nr_iowait_cpu(int cpu);
 extern unsigned long this_cpu_load(void);
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-extern unsigned long this_cpu_loadx(int i);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
+
 extern void calc_global_load(unsigned long ticks);
 
 extern unsigned long get_parent_ip(unsigned long addr);
@@ -399,7 +397,6 @@ extern void arch_unmap_area_topdown(struct mm_struct *, unsigned long);
 static inline void arch_pick_mmap_layout(struct mm_struct *mm) {}
 #endif
 
-
 extern void set_dumpable(struct mm_struct *mm, int value);
 extern int get_dumpable(struct mm_struct *mm);
 
@@ -728,7 +725,6 @@ extern struct user_struct *find_user(uid_t);
 extern struct user_struct root_user;
 #define INIT_USER (&root_user)
 
-
 struct backing_dev_info;
 struct reclaim_state;
 
@@ -1047,10 +1043,8 @@ partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 }
 #endif	/* !CONFIG_SMP */
 
-
 struct io_context;			/* See blkdev.h */
 
-
 #ifdef ARCH_HAS_PREFETCH_SWITCH_STACK
 extern void prefetch_stack(struct task_struct *t);
 #else
@@ -1317,9 +1313,9 @@ struct task_struct {
 	unsigned long stack_canary;
 #endif
 
-	/* 
+	/*
 	 * pointers to (original) parent process, youngest child, younger sibling,
-	 * older sibling, respectively.  (p->father can be replaced with 
+	 * older sibling, respectively.  (p->father can be replaced with
 	 * p->real_parent->pid)
 	 */
 	struct task_struct *real_parent; /* real parent process */
@@ -1669,7 +1665,6 @@ static inline pid_t task_pid_vnr(struct task_struct *tsk)
 	return __task_pid_nr_ns(tsk, PIDTYPE_PID, NULL);
 }
 
-
 static inline pid_t task_tgid_nr(struct task_struct *tsk)
 {
 	return tsk->tgid;
@@ -1682,7 +1677,6 @@ static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 	return pid_vnr(task_tgid(tsk));
 }
 
-
 static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,
 					struct pid_namespace *ns)
 {
@@ -1694,7 +1688,6 @@ static inline pid_t task_pgrp_vnr(struct task_struct *tsk)
 	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, NULL);
 }
 
-
 static inline pid_t task_session_nr_ns(struct task_struct *tsk,
 					struct pid_namespace *ns)
 {
@@ -1900,7 +1893,6 @@ extern u64 cpu_clock(int cpu);
 extern u64 local_clock(void);
 extern u64 sched_clock_cpu(int cpu);
 
-
 extern void sched_clock_init(void);
 
 #ifndef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
@@ -2142,7 +2134,7 @@ static inline int dequeue_signal_lock(struct task_struct *tsk, sigset_t *mask, s
 	spin_unlock_irqrestore(&tsk->sighand->siglock, flags);
 
 	return ret;
-}	
+}
 
 extern void block_all_signals(int (*notifier)(void *priv), void *priv,
 			      sigset_t *mask);
diff --git a/kernel/sched.c b/kernel/sched.c
index 0102a2d..4de213a 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -3287,14 +3297,6 @@ unsigned long this_cpu_load(void)
 	return this->cpu_load[0];
 }
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-unsigned long this_cpu_loadx(int i)
-{
-	struct rq *this = this_rq();
-	return this->cpu_load[i];
-}
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
-
 /* Variables and functions for calc_load */
 static atomic_long_t calc_load_tasks;
 static unsigned long calc_load_update;
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 00fefd0..8e743c7 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -55,6 +55,25 @@
 
 #include <trace/events/vmscan.h>
 
+struct cgroup_subsys mem_cgroup_subsys __read_mostly;
+#define MEM_CGROUP_RECLAIM_RETRIES	5
+struct mem_cgroup *root_mem_cgroup __read_mostly;
+
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR_SWAP
+/* Turned on only when memory cgroup is enabled && really_do_swap_account = 1 */
+int do_swap_account __read_mostly;
+
+/* for remember boot option*/
+#ifdef CONFIG_CGROUP_MEM_RES_CTLR_SWAP_ENABLED
+static int really_do_swap_account __initdata = 1;
+#else
+static int really_do_swap_account __initdata = 0;
+#endif
+
+#else
+#define do_swap_account		(0)
+#endif
+
 /*
  * Statistics for memory cgroup.
  */
@@ -435,7 +454,6 @@ mem_cgroup_remove_exceeded(struct mem_cgroup *mem,
 	spin_unlock(&mctz->lock);
 }
 
-
 static void mem_cgroup_update_tree(struct mem_cgroup *mem, struct page *page)
 {
 	unsigned long long excess;
@@ -819,7 +837,6 @@ static struct mem_cgroup *mem_cgroup_get_next(struct mem_cgroup *iter,
 #define for_each_mem_cgroup_all(iter) \
 	for_each_mem_cgroup_tree_cond(iter, NULL, true)
 
-
 static inline bool mem_cgroup_is_root(struct mem_cgroup *mem)
 {
 	return (mem == root_mem_cgroup);
@@ -1013,7 +1030,6 @@ static void mem_cgroup_lru_add_after_commit(struct page *page)
 	spin_unlock_irqrestore(&zone->lru_lock, flags);
 }
 
-
 void mem_cgroup_move_lists(struct page *page,
 			   enum lru_list from, enum lru_list to)
 {
@@ -1430,7 +1446,6 @@ void mem_cgroup_print_oom_info(struct mem_cgroup *memcg, struct task_struct *p)
 	if (!memcg || !p)
 		return;
 
-
 	rcu_read_lock();
 
 	mem_cgrp = memcg->css.cgroup;
@@ -1755,7 +1770,6 @@ static int mem_cgroup_hierarchical_reclaim(struct mem_cgroup *root_mem,
 			continue;
 		}
 		/* we use swappiness of local cgroup */
-
 		if (check_soft) {
 			ret = mem_cgroup_shrink_node_zone(victim, gfp_mask,
 				noswap, get_swappiness(victim), zone,
@@ -1815,7 +1829,6 @@ static int mem_cgroup_oom_unlock(struct mem_cgroup *mem)
 	return 0;
 }
 
-
 static DEFINE_MUTEX(memcg_oom_mutex);
 static DECLARE_WAIT_QUEUE_HEAD(memcg_oom_waitq);
 
@@ -2177,7 +2190,6 @@ static int __cpuinit memcg_cpu_hotplug_callback(struct notifier_block *nb,
 	return NOTIFY_OK;
 }
 
-
 /* See __mem_cgroup_try_charge() for details */
 enum {
 	CHARGE_OK,		/* success */
@@ -3870,7 +3882,6 @@ int mem_cgroup_force_empty_write(struct cgroup *cont, unsigned int event)
 	return mem_cgroup_force_empty(mem_cgroup_from_cont(cont), true);
 }
 
-
 static u64 mem_cgroup_hierarchy_read(struct cgroup *cont, struct cftype *cft)
 {
 	return mem_cgroup_from_cont(cont)->use_hierarchy;
@@ -3909,7 +3920,6 @@ static int mem_cgroup_hierarchy_write(struct cgroup *cont, struct cftype *cft,
 	return retval;
 }
 
-
 static unsigned long mem_cgroup_recursive_stat(struct mem_cgroup *mem,
 					       enum mem_cgroup_stat_index idx)
 {
@@ -4109,7 +4119,6 @@ static int mem_cgroup_move_charge_write(struct cgroup *cgrp,
 }
 #endif
 
-
 /* For read statistics */
 enum {
 	MCS_CACHE,
@@ -4151,7 +4160,6 @@ struct {
 	{"unevictable", "total_unevictable"}
 };
 
-
 static void
 mem_cgroup_get_local_stat(struct mem_cgroup *mem, struct mcs_total_stat *s)
 {
@@ -4254,7 +4262,6 @@ static int mem_control_stat_show(struct cgroup *cont, struct cftype *cft,
 	memset(&mystat, 0, sizeof(mystat));
 	mem_cgroup_get_local_stat(mem_cont, &mystat);
 
-
 	for (i = 0; i < NR_MCS_STAT; i++) {
 		if (i == MCS_SWAP && !do_swap_account)
 			continue;
@@ -5391,7 +5398,6 @@ static void mem_cgroup_clear_mc(void)
 	mem_cgroup_end_move(from);
 }
 
-
 static int mem_cgroup_can_attach(struct cgroup_subsys *ss,
 				struct cgroup *cgroup,
 				struct task_struct *p)
diff --git a/mm/shmem.c b/mm/shmem.c
index e17c52e..f014e35 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1060,17 +1060,8 @@ static int shmem_writepage(struct page *page, struct writeback_control *wbc)
 	info = SHMEM_I(inode);
 	if (info->flags & VM_LOCKED)
 		goto redirty;
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	/*
-	 * Modification for compcache
-	 * shmem_writepage can be reason of kernel panic when using swap.
-	 * This modification prevent using swap by shmem.
-	 */
-	goto redirty;
-#else
 	if (!total_swap_pages)
 		goto redirty;
-#endif
 
 	/*
 	 * shmem_backing_dev_info's capabilities prevent regular writeback or
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 7a6886a..4159bfe 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -117,11 +117,6 @@ struct scan_control {
 
 #define lru_to_page(_head) (list_entry((_head)->prev, struct page, lru))
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-atomic_t kswapd_thread_on = ATOMIC_INIT(1);
-extern int get_soft_reclaim_status(void);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
-
 #ifdef ARCH_HAS_PREFETCH
 #define prefetch_prev_lru_page(_page, _base, _field)			\
 	do {								\
@@ -183,7 +178,6 @@ static unsigned long zone_nr_lru_pages(struct zone *zone,
 	return zone_page_state(zone, NR_LRU_BASE + lru);
 }
 
-
 /*
  * Add a shrinker callback to be called from the vm
  */
@@ -737,10 +731,9 @@ static enum page_references page_check_references(struct page *page,
 #else
 		if (referenced_page || referenced_ptes > 1)
 			return PAGEREF_ACTIVATE;
-
 		/*
 		 * Activate file-backed executable pages after first usage.
-		*/
+		 */
 		if (vm_flags & VM_EXEC)
 			return PAGEREF_ACTIVATE;
 #endif
@@ -1359,13 +1349,6 @@ static int too_many_isolated(struct zone *zone, int file,
 {
 	unsigned long inactive, isolated;
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	if(get_soft_reclaim_status() == 1)
-	{
-		return 0;
-	}
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
-
 	if (current_is_kswapd())
 		return 0;
 
@@ -2090,10 +2073,6 @@ restart:
 		 */
 		if (nr_reclaimed >= nr_to_reclaim && priority < DEF_PRIORITY)
 			break;
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-		if ((sc->nr_reclaimed + nr_reclaimed) >= nr_to_reclaim && sc->may_swap)
-			break;
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	}
 	blk_finish_plug(&plug);
 	sc->nr_reclaimed += nr_reclaimed;
@@ -2164,17 +2143,18 @@ static inline bool compaction_ready(struct zone *zone, struct scan_control *sc)
 {
 	struct zoneref *z;
 	struct zone *zone;
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	unsigned long nr_soft_reclaimed = 0;
-#else
 	unsigned long nr_soft_reclaimed;
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	unsigned long nr_soft_scanned;
 	bool aborted_reclaim = false;
 
@@ -2213,11 +2193,9 @@ static bool shrink_zones(int priority, struct zonelist *zonelist,
 			 * and balancing, not for a memcg's limit.
 			 */
 			nr_soft_scanned = 0;
-#ifndef CONFIG_ZRAM_FOR_ANDROID
 			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,
 						sc->order, sc->gfp_mask,
 						&nr_soft_scanned);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 			sc->nr_reclaimed += nr_soft_reclaimed;
 			sc->nr_scanned += nr_soft_scanned;
 			/* need some check for avoid more shrink_zone() */
@@ -2376,11 +2354,8 @@ unsigned long try_to_free_pages(struct zonelist *zonelist, int order,
 		.may_writepage = !laptop_mode,
 		.nr_to_reclaim = SWAP_CLUSTER_MAX,
 		.may_unmap = 1,
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-		.may_swap = 0,
-#else
 		.may_swap = 1,
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
+		.swappiness = vm_swappiness,
 		.order = order,
 		.mem_cgroup = NULL,
 		.nodemask = nodemask,
@@ -2594,20 +2573,12 @@ static unsigned long balance_pgdat(pg_data_t *pgdat, int order,
 	int end_zone = 0;	/* Inclusive.  0 = ZONE_DMA */
 	unsigned long total_scanned;
 	struct reclaim_state *reclaim_state = current->reclaim_state;
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-	unsigned long nr_soft_reclaimed = 0;
-#else
 	unsigned long nr_soft_reclaimed;
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 	unsigned long nr_soft_scanned;
 	struct scan_control sc = {
 		.gfp_mask = GFP_KERNEL,
 		.may_unmap = 1,
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-		.may_swap = 0,
-#else
 		.may_swap = 1,
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 		/*
 		 * kswapd doesn't want to be bailed out while reclaim. because
 		 * we want to put equal scanning pressure on each zone.
@@ -2702,11 +2673,9 @@ loop_again:
 			/*
 			 * Call soft limit reclaim before calling shrink_zone.
 			 */
-#ifndef CONFIG_ZRAM_FOR_ANDROID
 			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,
 							order, sc.gfp_mask,
 							&nr_soft_scanned);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 			sc.nr_reclaimed += nr_soft_reclaimed;
 			total_scanned += nr_soft_scanned;
 
@@ -2902,11 +2871,6 @@ static void kswapd_try_to_sleep(pg_data_t *pgdat, int order, int classzone_idx)
 		 * per-cpu vmstat threshold while kswapd is awake and restore
 		 * them before going back to sleep.
 		 */
-
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-		atomic_set(&kswapd_thread_on,0);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
-
 		set_pgdat_percpu_threshold(pgdat, calculate_normal_threshold);
 
 		if (!kthread_should_stop())
@@ -3012,10 +2976,6 @@ static int kswapd(void *p)
 		if (kthread_should_stop())
 			break;
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-		atomic_set(&kswapd_thread_on, 1);
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
-
 		/*
 		 * We can speed up thawing tasks if we don't call balance_pgdat
 		 * after returning from the refrigerator
@@ -3027,7 +2987,6 @@ static int kswapd(void *p)
 						&balanced_classzone_idx);
 		}
 	}
-
 	current->reclaim_state = NULL;
 	return 0;
 }
@@ -3093,130 +3052,6 @@ unsigned long zone_reclaimable_pages(struct zone *zone)
 	return nr;
 }
 
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-/*
- * This is the main entry point to direct page reclaim for RTCC.
- *
- * If a full scan of the inactive list fails to free enough memory then we
- * are "out of memory" and something needs to be killed.
- *
- * If the caller is !__GFP_FS then the probability of a failure is reasonably
- * high - the zone may be full of dirty or under-writeback pages, which this
- * caller can't do much about.  We kick the writeback threads and take explicit
- * naps in the hope that some of these pages can be written.  But if the
- * allocating task holds filesystem locks which prevent writeout this might not
- * work, and the allocation attempt will fail.
- *
- * returns:	0, if no pages reclaimed
- * 		else, the number of pages reclaimed
- */
-static unsigned long rtcc_do_try_to_free_pages(struct zonelist *zonelist, struct scan_control *sc, struct shrink_control *shrink)
-{
-	int priority;
-	unsigned long total_scanned = 0;
-	struct zoneref *z;
-	struct zone *zone;
-	unsigned long writeback_threshold;
-	bool aborted_reclaim;
-
-	get_mems_allowed();
-	delayacct_freepages_start();
-
-	if (scanning_global_lru(sc))
-		count_vm_event(ALLOCSTALL);
-
-	for (priority = DEF_PRIORITY; priority >= 0; priority--) {
-		sc->nr_scanned = 0;
-		if (!priority)
-			disable_swap_token(sc->mem_cgroup);
-		shrink_zones(priority, zonelist, sc);
-		total_scanned += sc->nr_scanned;
-		if (sc->nr_reclaimed >= sc->nr_to_reclaim)
-			goto out;
-
-		/*
-		 * Try to write back as many pages as we just scanned.  This
-		 * tends to cause slow streaming writers to write data to the
-		 * disk smoothly, at the dirtying rate, which is nice.   But
-		 * that's undesirable in laptop mode, where we *want* lumpy
-		 * writeout.  So in laptop mode, write out the whole world.
-		 */
-		writeback_threshold = sc->nr_to_reclaim + sc->nr_to_reclaim / 2;
-		if (total_scanned > writeback_threshold) {
-			wakeup_flusher_threads(laptop_mode ? 0 : total_scanned);
-			sc->may_writepage = 1;
-		}
-
-		/* Take a nap, wait for some writeback to complete */
-		if (!sc->hibernation_mode && sc->nr_scanned &&
-		    priority < DEF_PRIORITY - 2) {
-			struct zone *preferred_zone;
-
-			first_zones_zonelist(zonelist, gfp_zone(sc->gfp_mask),
-						&cpuset_current_mems_allowed,
-						&preferred_zone);
-			wait_iff_congested(preferred_zone, BLK_RW_ASYNC, HZ/10);
-		}
-	}
-
-out:
-	delayacct_freepages_end();
-	put_mems_allowed();
-
-	if (sc->nr_reclaimed)
-		return sc->nr_reclaimed;
-
-	/*
-	 * As hibernation is going on, kswapd is freezed so that it can't mark
-	 * the zone into all_unreclaimable. Thus bypassing all_unreclaimable
-	 * check.
-	 */
-	if (oom_killer_disabled)
-		return 0;
-
-	/* top priority shrink_zones still had more to do? don't OOM, then */
-	if (scanning_global_lru(sc) && !all_unreclaimable(zonelist, sc))
-		return 1;
-
-	return 0;
-}
-
-long rtcc_reclaim_pages(long nr_to_reclaim)
-{
-	struct reclaim_state reclaim_state;
-	struct scan_control sc = {
-		.gfp_mask = GFP_KERNEL,
-		.may_swap = 1,
-		.may_unmap = 1,
-		.may_writepage = 1,
-		.nr_to_reclaim = nr_to_reclaim,
-		.swappiness = 100,
-		.order = 0,
-	};
-	struct shrink_control shrink = {
-		.gfp_mask = sc.gfp_mask,
-	};
-	struct zonelist *zonelist = node_zonelist(numa_node_id(), sc.gfp_mask);
-	struct task_struct *p = current;
-	unsigned long nr_reclaimed;
-
-	p->flags |= PF_MEMALLOC;
-	lockdep_set_current_reclaim_state(sc.gfp_mask);
-	reclaim_state.reclaimed_slab = 0;
-	p->reclaim_state = &reclaim_state;
-
-	nr_reclaimed = rtcc_do_try_to_free_pages(zonelist, &sc, &shrink);
-
-	p->reclaim_state = NULL;
-	lockdep_clear_current_reclaim_state();
-	p->flags &= ~PF_MEMALLOC;
-
-	printk("RTCC, reclaim %ld pages\n", nr_reclaimed);
-
-	return nr_reclaimed;
-}
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
-
 #ifdef CONFIG_HIBERNATION
 /*
  * Try to free `nr_to_reclaim' of memory, system-wide, and return the number of
@@ -3422,11 +3257,7 @@ static int __zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)
 	struct scan_control sc = {
 		.may_writepage = !!(zone_reclaim_mode & RECLAIM_WRITE),
 		.may_unmap = !!(zone_reclaim_mode & RECLAIM_SWAP),
-#ifdef CONFIG_ZRAM_FOR_ANDROID
-		.may_swap = 0,
-#else
 		.may_swap = 1,
-#endif /* CONFIG_ZRAM_FOR_ANDROID */
 		.nr_to_reclaim = max_t(unsigned long, nr_pages,
 				       SWAP_CLUSTER_MAX),
 		.gfp_mask = gfp_mask,
@@ -3712,7 +3543,6 @@ static void scan_zone_unevictable_pages(struct zone *zone)
 	}
 }
 
-
 /**
  * scan_all_zones_unevictable_pages - scan all unevictable lists for evictable pages
  *
@@ -3785,7 +3615,6 @@ static ssize_t write_scan_unevictable_node(struct sys_device *dev,
 	return 1;
 }
 
-
 static SYSDEV_ATTR(scan_unevictable_pages, S_IRUGO | S_IWUSR,
 			read_scan_unevictable_node,
 			write_scan_unevictable_node);
