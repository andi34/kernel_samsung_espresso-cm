From aa754d5580e211740bf063842707bd32db866306 Mon Sep 17 00:00:00 2001
From: andi34 <skate4life@gmx.de>
Date: Sat, 12 Jul 2014 16:25:18 +0200
Subject: [PATCH] arch/arm/mm/dma-mapping.c: revert JB Update 1/3

---
diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c
index a763366..1a765c8 100644
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@ -17,22 +17,13 @@
 #include <linux/init.h>
 #include <linux/device.h>
 #include <linux/dma-mapping.h>
-#include <linux/dma-contiguous.h>
 #include <linux/highmem.h>
-#include <linux/memblock.h>
-#include <linux/slab.h>
-#include <linux/kconfig.h>
 
 #include <asm/memory.h>
 #include <asm/highmem.h>
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>
 #include <asm/sizes.h>
-#include <asm/mach/arch.h>
-#include <asm/mach/map.h>
-#include <asm/dma-contiguous.h>
-
-#include "mm.h"
 
 static u64 get_coherent_dma_mask(struct device *dev)
 {
@@ -61,19 +52,6 @@ static u64 get_coherent_dma_mask(struct device *dev)
 	return mask;
 }
 
-static void __dma_clear_buffer(struct page *page, size_t size)
-{
-	void *ptr;
-	/*
-	 * Ensure that the allocated pages are zeroed, and that any data
-	 * lurking in the kernel direct-mapped region is invalidated.
-	 */
-	ptr = page_address(page);
-	memset(ptr, 0, size);
-	dmac_flush_range(ptr, ptr + size);
-	outer_flush_range(__pa(ptr), __pa(ptr) + size);
-}
-
 /*
  * Allocate a DMA buffer for 'dev' of size 'size' using the
  * specified gfp mask.  Note that 'size' must be page aligned.
@@ -82,6 +60,23 @@ static struct page *__dma_alloc_buffer(struct device *dev, size_t size, gfp_t gf
 {
 	unsigned long order = get_order(size);
 	struct page *page, *p, *e;
+	void *ptr;
+	u64 mask = get_coherent_dma_mask(dev);
+
+#ifdef CONFIG_DMA_API_DEBUG
+	u64 limit = (mask + 1) & ~mask;
+	if (limit && size >= limit) {
+		dev_warn(dev, "coherent allocation too big (requested %#x mask %#llx)\n",
+			size, mask);
+		return NULL;
+	}
+#endif
+
+	if (!mask)
+		return NULL;
+
+	if (mask < 0xffffffffULL)
+		gfp |= GFP_DMA;
 
 	page = alloc_pages(gfp, order);
 	if (!page)
@@ -94,7 +89,14 @@ static struct page *__dma_alloc_buffer(struct device *dev, size_t size, gfp_t gf
 	for (p = page + (size >> PAGE_SHIFT), e = page + (1 << order); p < e; p++)
 		__free_page(p);
 
-	__dma_clear_buffer(page, size);
+	/*
+	 * Ensure that the allocated pages are zeroed, and that any data
+	 * lurking in the kernel direct-mapped region is invalidated.
+	 */
+	ptr = page_address(page);
+	memset(ptr, 0, size);
+	dmac_flush_range(ptr, ptr + size);
+	outer_flush_range(__pa(ptr), __pa(ptr) + size);
 
 	return page;
 }
@@ -153,9 +155,6 @@ static int __init consistent_init(void)
 	int i = 0;
 	u32 base = CONSISTENT_BASE;
 
-	if (IS_ENABLED(CONFIG_CMA))
-		return 0;
-
 	do {
 		pgd = pgd_offset(&init_mm, base);
 
@@ -187,100 +186,8 @@ static int __init consistent_init(void)
 
 	return ret;
 }
-core_initcall(consistent_init);
-
-static void *__alloc_from_contiguous(struct device *dev, size_t size,
-				     pgprot_t prot, struct page **ret_page);
-
-static struct arm_vmregion_head coherent_head = {
-	.vm_lock	= __SPIN_LOCK_UNLOCKED(&coherent_head.vm_lock),
-	.vm_list	= LIST_HEAD_INIT(coherent_head.vm_list),
-};
-
-size_t coherent_pool_size = CONSISTENT_DMA_SIZE / 8;
-
-static int __init early_coherent_pool(char *p)
-{
-	coherent_pool_size = memparse(p, &p);
-	return 0;
-}
-early_param("coherent_pool", early_coherent_pool);
-
-/*
- * Initialise the coherent pool for atomic allocations.
- */
-static int __init coherent_init(void)
-{
-	pgprot_t prot = pgprot_dmacoherent(pgprot_kernel);
-	size_t size = coherent_pool_size;
-	struct page *page;
-	void *ptr;
-
-	if (!IS_ENABLED(CONFIG_CMA))
-		return 0;
-
-	ptr = __alloc_from_contiguous(NULL, size, prot, &page);
-	if (ptr) {
-		coherent_head.vm_start = (unsigned long) ptr;
-		coherent_head.vm_end = (unsigned long) ptr + size;
-		printk(KERN_INFO "DMA: preallocated %u KiB pool for atomic coherent allocations\n",
-		       (unsigned)size / 1024);
-		return 0;
-	}
-	printk(KERN_ERR "DMA: failed to allocate %u KiB pool for atomic coherent allocation\n",
-	       (unsigned)size / 1024);
-	return -ENOMEM;
-}
-/*
- * CMA is activated by core_initcall, so we must be called after it.
- */
-postcore_initcall(coherent_init);
-
-struct dma_contig_early_reserve {
-	phys_addr_t base;
-	unsigned long size;
-};
-
-static struct dma_contig_early_reserve dma_mmu_remap[MAX_CMA_AREAS] __initdata;
-
-static int dma_mmu_remap_num __initdata;
-
-void __init dma_contiguous_early_fixup(phys_addr_t base, unsigned long size)
-{
-	dma_mmu_remap[dma_mmu_remap_num].base = base;
-	dma_mmu_remap[dma_mmu_remap_num].size = size;
-	dma_mmu_remap_num++;
-}
 
-void __init dma_contiguous_remap(void)
-{
-	int i;
-	for (i = 0; i < dma_mmu_remap_num; i++) {
-		phys_addr_t start = dma_mmu_remap[i].base;
-		phys_addr_t end = start + dma_mmu_remap[i].size;
-		struct map_desc map;
-		unsigned long addr;
-
-		if (end > arm_lowmem_limit)
-			end = arm_lowmem_limit;
-		if (start >= end)
-			return;
-
-		map.pfn = __phys_to_pfn(start);
-		map.virtual = __phys_to_virt(start);
-		map.length = end - start;
-		map.type = MT_MEMORY_DMA_READY;
-
-		/*
-		 * Clear previous low-memory mapping
-		 */
-		for (addr = __phys_to_virt(start); addr < __phys_to_virt(end);
-		     addr += PGDIR_SIZE)
-			pmd_clear(pmd_off_k(addr));
-
-		iotable_init(&map, 1);
-	}
-}
+core_initcall(consistent_init);
 
 static void *
 __dma_alloc_remap(struct page *page, size_t size, gfp_t gfp, pgprot_t prot)
@@ -387,195 +294,36 @@ static void __dma_free_remap(void *cpu_addr, size_t size)
 	arm_vmregion_free(&consistent_head, c);
 }
 
-static int __dma_update_pte(pte_t *pte, pgtable_t token, unsigned long addr,
-			    void *data)
-{
-	struct page *page = virt_to_page(addr);
-	pgprot_t prot = *(pgprot_t *)data;
-
-	set_pte_ext(pte, mk_pte(page, prot), 0);
-	return 0;
-}
-
-static void __dma_remap(struct page *page, size_t size, pgprot_t prot)
-{
-	unsigned long start = (unsigned long) page_address(page);
-	unsigned end = start + size;
-
-	apply_to_page_range(&init_mm, start, size, __dma_update_pte, &prot);
-	dsb();
-	flush_tlb_kernel_range(start, end);
-}
-
-static void *__alloc_remap_buffer(struct device *dev, size_t size, gfp_t gfp,
-				 pgprot_t prot, struct page **ret_page)
-{
-	struct page *page;
-	void *ptr;
-	page = __dma_alloc_buffer(dev, size, gfp);
-	if (!page)
-		return NULL;
-
-	ptr = __dma_alloc_remap(page, size, gfp, prot);
-	if (!ptr) {
-		__dma_free_buffer(page, size);
-		return NULL;
-	}
-
-	*ret_page = page;
-	return ptr;
-}
-
-static void *__alloc_from_pool(struct device *dev, size_t size,
-			       struct page **ret_page)
-{
-	struct arm_vmregion *c;
-	size_t align;
-
-	if (!coherent_head.vm_start) {
-		printk(KERN_ERR "%s: coherent pool not initialised!\n",
-		       __func__);
-		dump_stack();
-		return NULL;
-	}
-
-	/*
-	 * Align the region allocation - allocations from pool are rather
-	 * small, so align them to their order in pages, minimum is a page
-	 * size. This helps reduce fragmentation of the DMA space.
-	 */
-	align = PAGE_SIZE << get_order(size);
-	c = arm_vmregion_alloc(&coherent_head, align, size, 0);
-	if (c) {
-		void *ptr = (void *)c->vm_start;
-		struct page *page = virt_to_page(ptr);
-		*ret_page = page;
-		return ptr;
-	}
-	return NULL;
-}
-
-static int __free_from_pool(void *cpu_addr, size_t size)
-{
-	unsigned long start = (unsigned long)cpu_addr;
-	unsigned long end = start + size;
-	struct arm_vmregion *c;
-
-	if (start < coherent_head.vm_start || end > coherent_head.vm_end)
-		return 0;
-
-	c = arm_vmregion_find_remove(&coherent_head, (unsigned long)start);
-
-	if ((c->vm_end - c->vm_start) != size) {
-		printk(KERN_ERR "%s: freeing wrong coherent size (%ld != %d)\n",
-		       __func__, c->vm_end - c->vm_start, size);
-		dump_stack();
-		size = c->vm_end - c->vm_start;
-	}
-
-	arm_vmregion_free(&coherent_head, c);
-	return 1;
-}
-
-static void *__alloc_from_contiguous(struct device *dev, size_t size,
-				     pgprot_t prot, struct page **ret_page)
-{
-	unsigned long order = get_order(size);
-	size_t count = size >> PAGE_SHIFT;
-	struct page *page;
-
-	page = dma_alloc_from_contiguous(dev, count, order);
-	if (!page)
-		return NULL;
-
-	__dma_clear_buffer(page, size);
-	__dma_remap(page, size, prot);
-
-	*ret_page = page;
-	return page_address(page);
-}
-
-static void __free_from_contiguous(struct device *dev, struct page *page,
-				   size_t size)
-{
-	__dma_remap(page, size, pgprot_kernel);
-	dma_release_from_contiguous(dev, page, size >> PAGE_SHIFT);
-}
-
-#define nommu() 0
-
 #else	/* !CONFIG_MMU */
 
-#define nommu() 1
-
-#define __alloc_remap_buffer(dev, size, gfp, prot, ret)	NULL
-#define __alloc_from_pool(dev, size, ret_page)		NULL
-#define __alloc_from_contiguous(dev, size, prot, ret)	NULL
-#define __free_from_pool(cpu_addr, size)		0
-#define __free_from_contiguous(dev, page, size)		do { } while (0)
-#define __dma_free_remap(cpu_addr, size)		do { } while (0)
+#define __dma_alloc_remap(page, size, gfp, prot)	page_address(page)
+#define __dma_free_remap(addr, size)			do { } while (0)
 
 #endif	/* CONFIG_MMU */
 
-static void *__alloc_simple_buffer(struct device *dev, size_t size, gfp_t gfp,
-				   struct page **ret_page)
-{
-	struct page *page;
-	page = __dma_alloc_buffer(dev, size, gfp);
-	if (!page)
-		return NULL;
-
-	*ret_page = page;
-	return page_address(page);
-}
-
-
-
-static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
-			 gfp_t gfp, pgprot_t prot)
+static void *
+__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp,
+	    pgprot_t prot)
 {
-	u64 mask = get_coherent_dma_mask(dev);
 	struct page *page;
 	void *addr;
 
-#ifdef CONFIG_DMA_API_DEBUG
-	u64 limit = (mask + 1) & ~mask;
-	if (limit && size >= limit) {
-		dev_warn(dev, "coherent allocation too big (requested %#x mask %#llx)\n",
-			size, mask);
-		return NULL;
-	}
-#endif
-
-	if (!mask)
-		return NULL;
-
-	if (mask < 0xffffffffULL)
-		gfp |= GFP_DMA;
-
-	/*
-	 * Following is a work-around (a.k.a. hack) to prevent pages
-	 * with __GFP_COMP being passed to split_page() which cannot
-	 * handle them.  The real problem is that this flag probably
-	 * should be 0 on ARM as it is not supported on this
-	 * platform; see CONFIG_HUGETLBFS.
-	 */
-	gfp &= ~(__GFP_COMP);
-
 	*handle = ~0;
 	size = PAGE_ALIGN(size);
 
-	if (arch_is_coherent() || nommu())
-		addr = __alloc_simple_buffer(dev, size, gfp, &page);
-	else if (!IS_ENABLED(CONFIG_CMA))
-		addr = __alloc_remap_buffer(dev, size, gfp, prot, &page);
-	else if (gfp & GFP_ATOMIC)
-		addr = __alloc_from_pool(dev, size, &page);
+	page = __dma_alloc_buffer(dev, size, gfp);
+	if (!page)
+		return NULL;
+
+	if (!arch_is_coherent())
+		addr = __dma_alloc_remap(page, size, gfp, prot);
 	else
-		addr = __alloc_from_contiguous(dev, size, prot, &page);
+		addr = page_address(page);
 
 	if (addr)
 		*handle = pfn_to_dma(dev, page_to_pfn(page));
+	else
+		__dma_free_buffer(page, size);
 
 	return addr;
 }
@@ -584,8 +332,8 @@ static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,
  * Allocate DMA-coherent memory space and return both the kernel remapped
  * virtual and bus address for that space.
  */
-void *dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *handle,
-			 gfp_t gfp)
+void *
+dma_alloc_coherent(struct device *dev, size_t size, dma_addr_t *handle, gfp_t gfp)
 {
 	void *memory;
 
@@ -616,13 +364,8 @@ EXPORT_SYMBOL(dma_alloc_writecombine);
 void *dma_alloc_stronglyordered(struct device *dev, size_t size,
 				dma_addr_t *handle, gfp_t gfp)
 {
-#ifdef CONFIG_CMA
-	struct page *page;
-	return __alloc_simple_buffer(dev, size, gfp, &page);
-#else
 	return __dma_alloc(dev, size, handle, gfp,
 			   pgprot_stronglyordered(pgprot_kernel));
-#endif
 }
 EXPORT_SYMBOL(dma_alloc_stronglyordered);
 
@@ -631,11 +374,25 @@ static int dma_mmap(struct device *dev, struct vm_area_struct *vma,
 {
 	int ret = -ENXIO;
 #ifdef CONFIG_MMU
-	unsigned long pfn = dma_to_pfn(dev, dma_addr);
-	ret = remap_pfn_range(vma, vma->vm_start,
-			      pfn + vma->vm_pgoff,
-			      vma->vm_end - vma->vm_start,
-			      vma->vm_page_prot);
+	unsigned long user_size, kern_size;
+	struct arm_vmregion *c;
+
+	user_size = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+
+	c = arm_vmregion_find(&consistent_head, (unsigned long)cpu_addr);
+	if (c) {
+		unsigned long off = vma->vm_pgoff;
+
+		kern_size = (c->vm_end - c->vm_start) >> PAGE_SHIFT;
+
+		if (off < kern_size &&
+		    user_size <= (kern_size - off)) {
+			ret = remap_pfn_range(vma, vma->vm_start,
+					      page_to_pfn(c->vm_pages) + off,
+					      user_size << PAGE_SHIFT,
+					      vma->vm_page_prot);
+		}
+	}
 #endif	/* CONFIG_MMU */
 
 	return ret;
@@ -657,33 +414,23 @@ int dma_mmap_writecombine(struct device *dev, struct vm_area_struct *vma,
 }
 EXPORT_SYMBOL(dma_mmap_writecombine);
 
-
 /*
- * Free a buffer as defined by the above mapping.
+ * free a page as defined by the above mapping.
+ * Must not be called with IRQs disabled.
  */
 void dma_free_coherent(struct device *dev, size_t size, void *cpu_addr, dma_addr_t handle)
 {
-	struct page *page = pfn_to_page(dma_to_pfn(dev, handle));
+	WARN_ON(irqs_disabled());
 
 	if (dma_release_from_coherent(dev, get_order(size), cpu_addr))
 		return;
 
 	size = PAGE_ALIGN(size);
 
-	if (arch_is_coherent() || nommu()) {
-		__dma_free_buffer(page, size);
-	} else if (!IS_ENABLED(CONFIG_CMA)) {
+	if (!arch_is_coherent())
 		__dma_free_remap(cpu_addr, size);
-		__dma_free_buffer(page, size);
-	} else {
-		if (__free_from_pool(cpu_addr, size))
-			return;
-		/*
-		 * Non-atomic allocations cannot be freed with IRQs disabled
-		 */
-		WARN_ON(irqs_disabled());
-		__free_from_contiguous(dev, page, size);
-	}
+
+	__dma_free_buffer(pfn_to_page(dma_to_pfn(dev, handle)), size);
 }
 EXPORT_SYMBOL(dma_free_coherent);
 
@@ -732,27 +479,25 @@ static void dma_cache_maint_page(struct page *page, unsigned long offset,
 	size_t size, enum dma_data_direction dir,
 	void (*op)(const void *, size_t, int))
 {
-	unsigned long pfn;
-	size_t left = size;
-
-	pfn = page_to_pfn(page) + offset / PAGE_SIZE;
-	offset %= PAGE_SIZE;
-
 	/*
 	 * A single sg entry may refer to multiple physically contiguous
 	 * pages.  But we still need to process highmem pages individually.
 	 * If highmem is not configured then the bulk of this loop gets
 	 * optimized out.
 	 */
+	size_t left = size;
 	do {
 		size_t len = left;
 		void *vaddr;
 
-		page = pfn_to_page(pfn);
-
 		if (PageHighMem(page)) {
-			if (len + offset > PAGE_SIZE)
+			if (len + offset > PAGE_SIZE) {
+				if (offset >= PAGE_SIZE) {
+					page += offset / PAGE_SIZE;
+					offset %= PAGE_SIZE;
+				}
 				len = PAGE_SIZE - offset;
+			}
 			vaddr = kmap_high_get(page);
 			if (vaddr) {
 				vaddr += offset;
@@ -769,7 +514,7 @@ static void dma_cache_maint_page(struct page *page, unsigned long offset,
 			op(vaddr, len, dir);
 		}
 		offset = 0;
-		pfn++;
+		page++;
 		left -= len;
 	} while (left);
 }
@@ -926,13 +671,6 @@ void dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
 }
 EXPORT_SYMBOL(dma_sync_sg_for_device);
 
-/*
- * Return whether the given device DMA address mask can be supported
- * properly.  For example, if your device can only drive the low 24-bits
- * during bus mastering, then you would pass 0x00ffffff as the mask
- * to this function.
- */
-
 #define PREALLOC_DMA_DEBUG_ENTRIES	4096
 
 static int __init dma_debug_do_init(void)
--
1.9.3


